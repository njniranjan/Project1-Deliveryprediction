# -*- coding: utf-8 -*-
"""Team4_MainProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jpWodmnYKTI5BDErHSqmAjT-AJQLZaUS

#Team 4: **Main Project**     
##Domain: **Supply Chain/Logistics**


##Dataset: **DataCoSupplychain**

##Importing all the necessary libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""## Step1: Reading file to Python enviornment"""

df=pd.read_csv("/content/drive/MyDrive/DataCoSupplyChainDataset.csv",encoding= 'unicode_escape')

df.head()

"""##Step2:Data Understanding

**Description of all the fields**
"""

des=pd.read_csv("/content/drive/MyDrive/DescriptionDataCoSupplyChain.csv")

des

##Get number of Rows and columns of dataset
df.shape

##Getting the concise summary of the dataset
df.info()

#Count number of distinct elements in columns
df.nunique()

"""##Step3: Data Preprocessing




1.   Filling the Missing
2.   Reducing the unwanted features(Noisy data)
3.   Finding outliers and eliminating them
4.   Split, merge or extract columns

##1:Missing value detection
"""

##Finding missing values
df.isna().sum()

##We will fill the Zipcode with mode value
df['Customer Zipcode']=df['Customer Zipcode'].fillna(df['Customer Zipcode'].mode)

df['Customer Zipcode'].isna().sum()

"""##Data Preprocessing
**2: Feature reduction**

**To make it easier for analysis some unimportant columns are dropped**
"""

df.columns

data=df.drop(['Category Id','Customer Email','Customer Fname','Customer Id','Customer Lname','Customer Password'], axis=1)

data.shape

data=data.drop(['Customer Street','Customer Zipcode','Department Id', 'Latitude', 'Longitude','Order Customer Id',], axis=1)

data.shape

data=data.drop(['Order Id','Order Item Cardprod Id','Order Zipcode','Product Card Id','Product Status','Product Category Id','Product Description','Product Image'], axis=1)

data.shape

"""**From above columns we can see 2 columns, 'days for shipment' and 'days for shipping' contribute to 'delivery status' column. Therefore we can reduce those two columns and keep only 'delivery status' column**"""

data=data.drop(['Days for shipping (real)','Days for shipment (scheduled)'], axis=1)

data.shape

data.info()

"""**Reduced number of columns from 53 to 31**

plotting correlation between all columns
"""

##plotting the matrix using heatmap
corrmatrix=data.corr()
plt.subplots(figsize=(20,15))
sns.heatmap(corrmatrix,vmin=-1,vmax=.7,annot=True,linewidth=0.2,cmap='YlGnBu')
plt.title('correlation Matrix of all columns',fontsize=18)

data.columns

"""**Feature reduction based on collinearity**

We can see 

      'Order Item Total' and  'Sales per customer',

       'Order Profit Per Order' and Benefit per order'

       Sales and   'Sales per customer'

       Order Item Profit Ratio' and Benefit per order'

       Order Item Product Price' and Sales per customer' are having high correlation.
**Features with high correlation is dropped**
"""

data=data.drop(['Order Profit Per Order','Order Item Total','Sales','Order Item Profit Ratio','Order Item Product Price'], axis=1)

data.shape

"""**The features are reduced from 53 to 26.**



"""

# to find the pairwise correlation of all columns in the dataframe
corrmatrix=data.corr()
plt.subplots(figsize=(20,15))
sns.heatmap(corrmatrix,vmin=-1,vmax=2,annot=True,linewidth=0.2,cmap='YlGnBu')

"""##Data Preprocessing
**3: Feature Engineering**

**Extracting 'Year','Month' columns from date of order column**
"""

##Extracting month and year from the date column

month_order=pd.DatetimeIndex(data['order date (DateOrders)']).month
year_order=pd.DatetimeIndex(data['order date (DateOrders)']).year

"""**Adding Month and year column to our dataset**"""

##Adding month_order column
data['month_order']=month_order

#Adding year_order column
data['year_order']=year_order

##Checking for newly added new col'umns 'year_order and 'month_order' to our data.
data.columns

"""**We can see both year_order and Month_order columns are added to our dataset**

**Dropping the 'order_date' and 'shipping date (DateOrders)' column from our dataset**
"""

data=data.drop(['order date (DateOrders)','shipping date (DateOrders)'],axis=1)

##To check the data type of new columns
data.dtypes

data.info()

##Todisplay maximum number of rows
pd.set_option('display.max_rows', None)

"""##Data Preprocessing
**4: Outlier detection**
"""

data.columns

##Gives all statistical values of our data set
data.describe()

"""From above description we can see that STD is high in columns **'Benefit per order', 'Sales per customer'** and

**'Order Item Discount'**

**We will plot Boxplot for all the three columns 'Benefit per order', 'Sales per customer'**  **'Order Item Discount'**
"""

##Box plot for Benefit per order column
plt.figure(figsize=(12,6))

sns.boxplot(data['Benefit per order'],color='red')
plt.title('Box plot for Benefit per order column',fontsize=18)

"""**We can see There are many outliers in the above boxplot of 'Benefit per order' column"""

data['Benefit per order'].describe()

"""**We will find different Quartile value of Benefit per order**"""

Q1=data['Benefit per order'].quantile(0.25)
Q2=data['Benefit per order'].quantile(0.50)
Q3=data['Benefit per order'].quantile(0.75)


##calculating inter quartile range
IQR=Q3-Q1

##Finding upperlimit and lower limit of data points
upp_limit=Q3+1.5*IQR
low_limit=Q1-1.5*IQR
print(upp_limit,low_limit)

##we will find outliers in the column using a for loop
##create a list named- outlier
outlier=[]
for x in(data['Benefit per order']):
    if (x>upp_limit)|(x<low_limit):
        outlier.append(x)
print(outlier)

##Finding the outliers in our data
data[(data['Benefit per order']<low_limit)|(data['Benefit per order']>upp_limit)]

##Data which doesnot have any outliers
data_benefit_out=data[(data['Benefit per order']>low_limit)&(data['Benefit per order']<upp_limit)]

data_benefit_out.columns

##plotting boxplot after taking out the outliers
plt.figure(figsize=(12,6))

sns.boxplot(data_benefit_out['Benefit per order'],color='green')
plt.title('Box plot for Benefit per order column after removing outliers',fontsize=18)

"""**Now, we can see all the outliers are removed**"""

data_benefit_out['Benefit per order'].describe()

"""**Here we can see that the min value is -79 and max is 151. And we have removed all the outliers**

##Boxplot for 'Sales per customer' column
"""

##Box plot for 'Sales per customer' column
plt.figure(figsize=(12,6))
sns.boxplot(data['Sales per customer'],color='red')
plt.title('Boxplot for Sales per customer',fontsize=18)

plt.figure(figsize=(12,6))

plt.hist(data['Sales per customer'])

data['Sales per customer'].describe()

Q1=data['Sales per customer'].quantile(0.25)
Q2=data['Sales per customer'].quantile(0.50)
Q3=data['Sales per customer'].quantile(0.75)


##calculating inter quartile range
IQR=Q3-Q1

##Finding upperlimit and lower limit of data points
upp_limit=Q3+1.5*IQR
low_limit=Q1-1.5*IQR
print(upp_limit,low_limit)

##Data which doesnot have any outliers
data_benefit_out=data_benefit_out[(data_benefit_out['Sales per customer']>low_limit)]

"""From the above graph we can see that few customers have ordered too many items. Since it is sales, it can be possible. Therefore we wont eliminate the outliers above the upper limit.

"""

plt.figure(figsize=(12,6))
sns.boxplot(data_benefit_out['Sales per customer'],color='green')
plt.title('Boxplot for Sales per customer after eliminating negetive values',fontsize=18)

"""### Boxplot for 'Order Item Discount'"""

plt.figure(figsize=(12,6))
sns.boxplot(data_benefit_out['Order Item Discount'],color='red')
plt.title('Boxplot for Order Item Discount',fontsize=18)

data_benefit_out['Order Item Discount'].describe()

"""From the above values we can see, Min Discount is 0.00 and Max is 500.00. It can be possible and cannot be considered as incorrectly entered data.

##creating a seperate dataset without outliers if required for any visualization
"""

##Finding the Quartiles of 'Order Item Discount'
Q1=data_benefit_out['Order Item Discount'].quantile(0.25)
Q2=data_benefit_out['Order Item Discount'].quantile(0.50)
Q3=data_benefit_out['Order Item Discount'].quantile(0.75)


##calculating inter quartile range
IQR=Q3-Q1

##Finding upperlimit and lower limit of data points
upp_limit=Q3+1.5*IQR
low_limit=Q1-1.5*IQR
print(upp_limit,low_limit)

##we will find outliers in the column using a for loop
##create a list named- outlier
outlier=[]
for x in(data_benefit_out['Order Item Discount']):
    if (x>upp_limit)|(x<low_limit):
        outlier.append(x)
print(outlier)

##Data which doesnot have any outliers
data_benefit_Disc_out=data[(data['Order Item Discount']>low_limit)&(data['Order Item Discount']<upp_limit)]

"""Boxplot for 'Order Item Discount' after removing outliers"""

plt.figure(figsize=(12,6))
sns.boxplot(data_benefit_Disc_out['Order Item Discount'],color='green')
plt.title('Boxplot for Order Item Discount after removing outliers',fontsize=18)

"""**All the outliers are removed**"""

data_benefit_Disc_out['Order Item Discount'].describe()

"""**After removing the outliers, we can see the max value has changed to 61 from 500.**

#Step4: Exploratory Data Analysis

##Numerical features
"""

data.info()

##creating a list of numerical features
numerical_features=data.select_dtypes(include=['float'])
numerical_features.hist(figsize=(20,15))
plt.title('Histogram of All Numerical Features',fontsize=18)
plt.show

numerical_features.shape

"""##Univariate,Bivariate and Multivariate Analysis

#Visualizations to understand Sales
"""

data_benefit_out.columns

data_benefit_out.groupby('Market')['Sales per customer'].sum()

"""## Figure1:
**Sum of Sales of each Market**
"""

data_benefit_out.groupby('Market')['Sales per customer'].sum().sort_values(ascending=False).plot.bar(figsize=(12,6))
plt.xlabel('Market to which product delivered')
plt.ylabel('Sum of Sales')
plt.title('Sum of Sales of All Markets',fontsize=18)

"""The Maximum order delivered Market is Europe and LATAM

##Sales in each year
"""

data_benefit_out.columns

data_benefit_out['year_order'].describe()

##plotting bargraph to plot Sum of Sales of each year.
data_benefit_out.groupby('year_order')['Sales per customer'].sum().sort_values(ascending=False).plot.bar(figsize=(12,6))
plt.xlabel('year')
plt.ylabel('sum of sales per customer')
plt.title('Sum of Sales of each year',fontsize=18)

##Mean of Sales in each Month
data_benefit_out.groupby('month_order')['Sales per customer'].mean().plot.bar(figsize=(12,6))
plt.title('Average of Sales in each Month',fontsize=18)

"""**All the Months have good Sales and it peaks in September**

##Figure2:
**Total Sales per Category**
"""

data_benefit_out['Category Name'].nunique()

sum=data_benefit_out.groupby('Category Name')['Sales per customer'].sum().sort_values(ascending=False).plot.bar(figsize=(12,6),title='Total sales per category')
plt.ylabel('Sales')

"""**Large Number of Sales per customer is done in**

1.   Fishing Category
2.   Cleats
3.   Camping and Hiking
4.   Women's apparel
5.   Water sports
6.   Cardio Equipment etc...

**and least sale is for**

 Books and Baby categories

##Figure 3:
**Mode of payment**
"""

plt.figure(figsize=(12,6))
plt.hist(data_benefit_out['Type'],color='green')
plt.title('Frequency of Mode of Payments used',fontsize=18)
plt.xlabel('Pament type')
plt.ylabel('Total count')
plt.show()

"""**The most used ModeOfPayment is Debit payment and Least used Mode of Payment is By Cash**

###Figure4:
**Top Products and Regions with negative revenue**
"""

loss = data[(data['Benefit per order']<0)]
#Plotting top 10 products with most loss
plt.figure(1)
loss['Category Name'].value_counts().nlargest(10).plot.bar(figsize=(20,8), title="Products with most loss")
plt.figure(2)
loss['Order Region'].value_counts().nlargest(10).plot.bar(figsize=(20,8), title="Regions with most loss")
#Sum of total sales which are lost
print('Total revenue lost with orders',loss['Benefit per order'].sum())

"""The total loss sales are approximately 1.1 Millions which is an huge amount.It can be seen that Cleats is the category with most loss sales followed by Mens footwear.Most lost sales are happeing in central america and western europe region.This lost sales may have happened due to suspected frauds or late deliveries.

##Total number of sales and Total Amount of sales per customer segment
"""

##graphs to visualize Total number of sales for each customer segment
plt.figure(figsize=(12,6))
plt.hist(data_benefit_out['Customer Segment'],color='grey')
plt.title('Number of Sales of Each Customer Segment',fontsize=18,c='Green')
plt.xlabel('Customer type')
plt.ylabel('Total number of sales')

data_benefit_out.columns

"""##Scatter plot to visualize average monthly wise Sales in each year"""

##slicing the dataframe and creating grp1 with year,month and sales per customer
grp1=data[['month_order','Sales per customer','year_order']]
##We are grouping by year to see monthwise sales of each year
grpSales=grp1.groupby(['year_order','month_order'],as_index=False)['Sales per customer'].mean()
grpSales

##plotting scatterplot from seaborn
sns.scatterplot(x=grpSales['year_order'],y=grpSales['month_order'],hue=grpSales['Sales per customer'],palette='Blues')
plt.title('Average MonthlyWise Sales',fontsize=18,c='Red')
plt.show

"""**Maximum Average Sales per customer is done in the year 2017 in the month of October**"""

sns.boxplot(y=data_benefit_out['Order Item Quantity'],x=data_benefit_out['Market'])
plt.title('Quantity of products per order',fontsize=18)

"""**Sum of Sales of each customer segment**"""

##Visualization of customer segment and Total Sales
data_benefit_out.groupby('Customer Segment')['Product Price'].sum().plot.pie(figsize=(10,10),autopct='%.1f')
plt.xlabel('Customer segment')
plt.ylabel('Sum of Sales')
plt.title('Customer Segment and Sum Of Sales',fontsize=18,c='Green')

"""**Consumer Segment has done Max orders and Also have Max Sale values**

#Visualizations for Late Delivery

**Departments of Most LateDelivery occurs**
"""

LateDel_Data=data_benefit_out[data_benefit_out['Delivery Status']=='Late delivery']

# to find in which department more late delivery occurs
LateDel_Data['Department Name'].value_counts().nlargest(10).plot.bar(figsize=(20,8))
plt.title('Deparments in which more late delivery occurs',fontsize=18,c='Green')

"""Most Late Delivery occurs in FanShop followed by Apparels

##Order status and corresponding counts
"""

data_benefit_out.columns

data_benefit_out['Order Status'].value_counts().plot.pie(figsize=(8,8),autopct='%.1f')
plt.title('Order status percentage',fontsize=18,c='Green')

"""##Delivery Status Visualization"""

data_benefit_out.columns

##Histogram of Delivery status column
plt.figure(figsize=(10,6))
plt.hist(data_benefit_out['Delivery Status'],color='green')
plt.title('Histogram of Delivery Status',size=18)
plt.xlabel('Delivery Status',size=14)
plt.ylabel('Count',size=14)
plt.show()

"""**We can see There are more number of Late Deliveries**

### Figure:
**Category of products being delivered late the most**
"""

#Filtering columns with late delivery status
late_delivery = data[(data['Delivery Status'] == 'Late delivery')]
#Top 10 products with most late deliveries
late_delivery['Category Name'].value_counts().nlargest(10).plot.bar(figsize=(20,8))
plt.title("Top 10 products with highest late deliveries",size=18,color='r')
plt.xlabel('Products',size=16,color='b')
plt.ylabel('Count',size=16,color='b')
plt.show()

"""It can be seen that orders with Cleats department is getting delayed the most followed by Men's Footwear.

## Visualization of Late Delivery Departments
"""

LateDel_Data=data_benefit_out[data_benefit_out['Delivery Status']=='Late delivery']
LateDel_Data.head()

"""**We can see FanShop is the most late delivered item**

###Figure:
**Number of late deliveries for each shipment methods**
"""

#Filtering late delivery orders with standard class shipping
xyz1 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'Standard Class')]
#Filtering late delivery orders with first class shipping
xyz2 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'First Class')]
#Filtering late delivery orders with second class shipping
xyz3 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'Second Class')]
#Filtering late delivery orders with same day shipping
xyz4 = data[(data['Delivery Status'] == 'Late delivery') & (data['Shipping Mode'] == 'Same Day')]
#Counting total values
count1=xyz1['Order Region'].value_counts()
count2=xyz2['Order Region'].value_counts()
count3=xyz3['Order Region'].value_counts()
count4=xyz4['Order Region'].value_counts()
#Index names
names=data['Order Region'].value_counts().keys()
n_groups=23
fig,ax = plt.subplots(figsize=(20,8))
index=np.arange(n_groups)
bar_width=0.2
opacity=0.6
type1=plt.bar(index,count1,bar_width,alpha=opacity,color='b',label='Standard Class')
type2=plt.bar(index+bar_width,count2,bar_width,alpha=opacity,color='r',label='First class')
type3=plt.bar(index+bar_width+bar_width,count3,bar_width,alpha=opacity,color='g',label='second class')
type4=plt.bar(index+bar_width+bar_width+bar_width,count4,bar_width,alpha=opacity,color='y',label='same day')
plt.xlabel('Order Regions',size=14,color='b')
plt.ylabel('Number of shipments',size=14,color='b')
plt.title('Late Deliveries for each shipment method in all regions',size=18,color='r')
plt.legend()
plt.xticks(index+bar_width,names,rotation=90)
plt.tight_layout()
plt.show()

"""As expected the most number of late deliveries for all regions occured with standard class shipping,with same day shipping being the one with least number of late deliveries.Both the first class and second class shipping have almost equal number of late deliveries.

##Which Shipment mode have most late delivey
"""

LateDel_Data.columns

##Shipment mode and late Delivery count
plt.figure(figsize=(12,6))
sns.histplot(LateDel_Data['Shipping Mode'])
plt.xlabel('Shipping mode')
plt.ylabel('')
plt.title('Shipping modes and Late Delivery',fontsize=18,c='Green')

"""**Standard class Shipping mode has most Late Delivery**

##Count of Late Delivery,OnTime Delivery and Cancelled shipment of each ShipmentMode
"""

##count of delivery status after grouping the shipment mode
status=data_benefit_out.groupby('Shipping Mode')['Delivery Status'].value_counts().plot.bar(figsize=(12,6))
plt.title('Shipping modes and count of Each Delivery Status',fontsize=18,c='Green')

"""Late Delivery and Cancellation order status is maximum for Standard class shipment mode

Shipping OnTime is max for Second class shipment mode

Least number of cancellation is for Same Day shipment mode

##Which Country has most Late Delivery
"""

# to find in which ordered countries more late delivery occurs
LateDel_Data['Order Country'].value_counts().nlargest(10).plot.bar(figsize=(20,8))
plt.title('Countries in which more late delivery occurs',fontsize=18,c='Green')

# to find in which city more late delivery occurs
LateDel_Data['Order City'].value_counts().nlargest(10).plot.pie(figsize=(7,7), autopct='%.1f')
plt.title('Cities in which more late delivery occurs',fontsize=18,c='Green')

"""**Most Late Delivery occurs in NewYork**

##Late Delivery Distribution plot
"""

##Distplot from seaborn library
plt.figure(figsize=(12,6))
sns.distplot(data_benefit_out['Late_delivery_risk'])
plt.title('Late Delivery Distribution plot',fontsize=18,c='Green')

sns.distplot(data_benefit_out['Product Price'])
plt.title('Distribution of productPrice',fontsize=18,c='Green')

"""##Bivariate Analysis"""

##correlation matrix of dataset
plt.subplots(figsize=(20,15))
sns.heatmap(data.corr(),vmax=.7,linewidth=.2,annot=True,cmap='Greens')
plt.title('Correlation Matrix',fontsize=18,c='Red')
plt.show()

"""**We will try to understand the relationship between Sales per Customer and Product price**"""

sns.relplot(x=data_benefit_out['Product Price'],y=data_benefit_out['Sales per customer'],hue=data_benefit_out['Product Name'])
plt.title('relationship between Product Price and Sales per customer',fontsize=18,c='Red')

data.columns

##slicing the dataframe and creating grp1 with year,month and sales per customer
grp1=data[['month_order','Sales per customer','year_order']]
##We are grouping by year to see monthwise sales of each year
grpSales=grp1.groupby(['year_order','month_order'],as_index=False)['Sales per customer'].mean()
grpSales

"""##Data Preprocessing
**5:Label Encoding our categorical Features before Modelling**
"""

##Label encoding all the categorical features
data_benefit_out.info()

##selecting all categorical features 
categorical_features= [feature for feature in data_benefit_out.columns if data_benefit_out[feature].dtypes=='O']
categorical_features

#Applying Label Encoder to Categorical Features
from sklearn.preprocessing import LabelEncoder
for feature in categorical_features:
    le = LabelEncoder()   
    data_benefit_out[feature]= le.fit_transform(data_benefit_out[feature])

data_benefit_out.head()

"""**Label Encoded all our categorical features**

#Step6: Modelling
"""

data_benefit_out.columns

"""**#Creating a new DataFrame for Late Delivery Prediction**"""

data_pred=data_benefit_out[['Late_delivery_risk','Customer City','Department Name','Order Country','Order City','Order Item Quantity','Order Status','Product Name','Shipping Mode','month_order','year_order']]

data_deploy=data_benefit_out[['Late_delivery_risk','Customer City','Order City','Order Status','Shipping Mode']]

data_deploy.to_csv("supplychainDataset.csv",index=False)

data_deploy.nunique()

data_pred.info()

data_pred.shape

"""**Seperating Target(y) and Feature(x)**"""

x=data_pred.drop(['Late_delivery_risk'],axis=1)
y=pd.DataFrame(data_pred['Late_delivery_risk'])

"""##Scaling our data before modelling"""

##importing standard scalar
from sklearn.preprocessing import StandardScaler
scalar=StandardScaler()
x_scaled=pd.DataFrame(scalar.fit_transform(x))

x.head()

##view our scaled data
x_scaled.describe()

"""##Reducing features using PCA"""

from sklearn.decomposition import PCA

data_benefit_out.info()

length=len(data_benefit_out.columns)
print(length)

##importing standard scalar
from sklearn.preprocessing import StandardScaler
scalar=StandardScaler()
X_pca_scaled=pd.DataFrame(scalar.fit_transform(data_benefit_out))

print(X_pca_scaled)

"""**PC loading**"""

##These are the vectors
PCA().fit(X_pca_scaled).components_

##These are the PCA loadings
PCA().fit(X_pca_scaled).components_.T

list1=np.arange(1,length+1)
print(list1)

##These are the major PCA loadings
pca_loadings=pd.DataFrame(PCA().fit(X_pca_scaled).components_.T,index=data_benefit_out.columns, columns=list1)

pca_loadings

X_pca_scaled.index

# Fit the PCA model and transform X to get the principal components
pca = PCA(n_components=4)
df_plot = pd.DataFrame(pca.fit_transform(X_pca_scaled),columns=['PC1','PC2','PC3','PC4'],index=X_pca_scaled.index)
df_plot

##This is the standard deviation of our Principal components
np.sqrt(pca.explained_variance_)

##The correlation between the Pricipal components is zero
df_plot.corr().round()

##How much data is present there in the PC1 and PC2
pca.explained_variance_ratio_

##to find the percentage of data in PCA
pca.explained_variance_ratio_*100

"""**Splitting our data into Train and Test set**"""

##importing necessary libraries
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,classification_report

##importing train_test_split from sklearn.model_selection
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x_scaled,y,test_size=0.3,random_state=42)

x_train.shape

x_test.shape

y_test.shape

y_train.shape

"""**1.K Nearest Neighbors**"""

#knn
from sklearn.neighbors import KNeighborsClassifier
knn_model=KNeighborsClassifier()
knn_model.fit(x_train,y_train)
y_pred=knn_model.predict(x_test)

print('Accuracy is :',accuracy_score(y_test,y_pred)*100)
print('Precision is :',precision_score(y_test,y_pred,average='micro'))
print('Recall is :',recall_score(y_test,y_pred,average='micro'))
print('f1_score is :',f1_score(y_test,y_pred,average='micro'))

"""**Hyperparameter tuning**"""

from sklearn.model_selection import GridSearchCV

#List Hyperparameters that we want to tune.
leaf_size = list(range(1,10))
n_neighbors = list(range(1,10))
p=[1,2]
#Convert to dictionary
hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)
#Create new KNN object
knn_2 = KNeighborsClassifier()
#Use GridSearch
clf = GridSearchCV(knn_2, hyperparameters, cv=10)
#Fit the model
best_model = clf.fit(x,y)
#Print The value of best Hyperparameters
print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])
print('Best p:', best_model.best_estimator_.get_params()['p'])
print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])

"""**2: Decision Tree Classifier**"""

##importing decison tree classifier from sklearn library
from sklearn.tree import DecisionTreeClassifier

##Creating an instance of Decision Tree Classifier
dt_model=DecisionTreeClassifier()

##Fitting our X-train and y_train to the model
dt_model.fit(x_train,y_train)

##Predicting the result using product method
y_pred = dt_model.predict(x_test)

##Calculating and printing Accuracy, Precision, Recall Score and f1_Score
print("accuracy",accuracy_score(y_test,y_pred))
print("Precision Score",precision_score(y_test,y_pred,average='macro'))
print("recall score",recall_score(y_test,y_pred,average='macro'))
print("f1 score",f1_score(y_test,y_pred,average='macro'))

y_test.value_counts()

confusion_matrix(y_test,y_pred)

"""**82% Accuracy is there for Decision Tree Algorithm**

**Fine Tuning of Decision Tree**
"""

##Fine tuning the parameters
model=DecisionTreeClassifier(criterion='gini',max_depth=300,max_features=6,min_samples_leaf=8, max_leaf_nodes=5)
model.fit(x_train,y_train)
y_pred =model.predict(x_test)
print(classification_report(y_test,y_pred))
print("Accuracy is : ",accuracy_score(y_test, y_pred))

"""**The Acuracy is high by using the default parameters**

**3: Random Forest**
"""

#RandomForest
from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier()
rf_model.fit(x_train,y_train)
y_pred = rf_model.predict(x_test)
print(classification_report(y_test,y_pred))
print("Accuracy is : ",accuracy_score(y_test, y_pred))

"""**Fine tuning of Random Forest Model**"""

from sklearn.model_selection import GridSearchCV, StratifiedKFold

clf = RandomForestClassifier(n_jobs=-1)

param_grid = {
    'min_samples_split': [3, 5, 10], 
    'n_estimators' : [100, 300],
    'max_depth': [3, 5, 15, 25],
    'max_features': [3, 5, 10, 20]
}

scorers = {
    'precision_score': make_scorer(precision_score),
    'recall_score': make_scorer(recall_score),
    'accuracy_score': make_scorer(accuracy_score)
}

def grid_search_wrapper(refit_score='accuracy_score'):
    """
    fits a GridSearchCV classifier using refit_score for optimization
    prints classifier performance metrics
    """
    skf = StratifiedKFold(n_splits=10)
    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,
                           cv=skf, return_train_score=True, n_jobs=-1)
    grid_search.fit(x_train.values, y_train.values)

    # make the predictions
    y_pred = grid_search.predict(x_test.values)

    print('Best params for {}'.format(refit_score))
    print(grid_search.best_params_)
    
    return grid_search

##feature importance
pd.Series(rf_model.feature_importances_,index=x.columns).sort_values(ascending=False)*100

"""**Manual Fine Tuning of hyperparameter**"""

model=RandomForestClassifier(n_estimators=300,max_depth=50,max_features=6,min_samples_leaf=4)
model.fit(x_train,y_train)
y_pred =model.predict(x_test)
print(classification_report(y_test,y_pred))
print("Accuracy is : ",accuracy_score(y_test, y_pred))

"""**4:Gradient Boosting Modelling**"""

##importing the necessary library
from sklearn.ensemble import GradientBoostingClassifier

##creating an instance for GradientBoostingClassifier
gbc=GradientBoostingClassifier()
def gradientPredict(x_train,x_test,y_train,y_test):
  ##Fitting our X-train and y_train to the model
  gbc.fit(x_train,y_train)
  ##predicting y values using predict method
  y_pred=gbc.predict(x_test)
  print("Accuracy of Gradient Boosting",accuracy_score(y_test,y_pred))
  print("Precision of Gradient Boosting:",precision_score(y_test,y_pred))
  print("Recall score of Gradient boosting:",recall_score(y_test,y_pred))
  print("f1 Score:",f1_score(y_test,y_pred))
  print("confusion Matrix")
  confusion_matrix(y_test,y_pred)

print("Accuracy and other scores:")
print("--------------------------")
gradientPredict(x_train,x_test,y_train,y_test)

"""**19503 Late delivery's are predicted correctly 2448 are predicted wrongly**

**Fine Tuning of Gradient Boosting**
"""

gbc=GradientBoostingClassifier(max_depth=10, min_samples_split=1000, min_samples_leaf=10)

print("Accuracy and f1 score after Fine Tunung:")
print("========================================")
gradientPredict(x_train,x_test,y_train,y_test)

"""**After fine Tuning of parameters, the Accuracy increased from 71% to 72%**

**5: Extreme Gradient Boosting**
"""

##importing the classifier
from xgboost import XGBClassifier
##creating an instance
xgb=XGBClassifier()

##Defining a function for prediction and finding all the accuracy scores
def xgbPredict(x_train,x_test,y_train,y_test):

  y_pred=xgb.predict(x_test)
  print("Accuracy of Extreme Gradient Boosting",accuracy_score(y_test,y_pred))
  print("Precision of Extreme Gradient Boosting:",precision_score(y_test,y_pred))
  print("Recall score of Extreme Gradient boosting:",recall_score(y_test,y_pred))
  print("f1 Score:",f1_score(y_test,y_pred))
  print("confusion Matrix")
  confusion_matrix(y_test,y_pred)

print("Accuracy and f1 scores of Extreme Gradient Boosting:")
print("========================================")
##Fitting the data to the model
xgb.fit(x_train,y_train)
##calling function to predict and Accuracy scores
xgbPredict(x_train,x_test,y_train,y_test)

"""**Fine Tuning parameters of Extreme Gradient Bosting**"""

print("Accuracy and f1 score after Fine Tunung:")
print("========================================")
xgb=XGBClassifier(max_depth=8,n_estimators=500,random_state=42)
xgb.fit(x_train,y_train)
xgbPredict(x_train,x_test,y_train,y_test)

"""**After fine tuning the parameters, The Accuracy increased from 71% to 78% and**

**f1-Score increased from 68% to 78%**

**6: Logistic Regression**
"""

from sklearn.linear_model import LogisticRegression
lr_model = LogisticRegression()
lr_model.fit(x_train,y_train)
y_pred_lr=lr_model.predict(x_test)

print(classification_report(y_test,y_pred_lr))

from sklearn.metrics import classification_report
matrix_data = confusion_matrix(y_test,y_pred_lr)

print(matrix_data)

"""**7: SVM classifier** ##SVM model was consuming too much time for execution """

##creating instance for linear SVC 
svm_linear=SVC(kernel='linear')

##Fitting our data to SVM linear model
svm_linear.fit(x_train,y_train)

##predicting y values
y_pred=svm_linear.predict(x_test)

##Calculating and printing Accuracy, Precision, Recall Score and f1_Score
print("accuracy",accuracy_score(y_test,y_pred))
print("Precision Score",precision_score(y_test,y_pred,average='macro'))
print("recall score",recall_score(y_test,y_pred,average='macro'))
print("f1 score",f1_score(y_test,y_pred,average='macro'))

##confusion matrix for SVM-linear kernel
confusion_matrix(y_test,y_pred)

"""**Polynomial SVM**"""

svm_poly = SVC(kernel='poly',degree=3)
svm_poly.fit(x_train,y_train)

y_pred=svm_poly.predict(x_test)

y_pred

##Calculating and printing Accuracy, Precision, Recall Score and f1_Score
print("Accuracy using polynomial SVM",accuracy_score(y_test,y_pred))
print("Precision using polynomial SVM",precision_score(y_test,y_pred))
print("Recall using polynomial SVM",recall_score(y_test,y_pred))
print("f1 score using polynomial SVM",f1_score(y_test,y_pred))

confusion_matrix(y_test,y_pred)

"""**Radial SVM**"""

##creating the instance
svm_radial=SVC(kernel='rbf')
svm_radial.fit(x_train,y_train)

y_pred=svm_radial.predict(x_test)

y_pred

##Calculating and printing Accuracy, Precision, Recall Score and f1_Score
print("Accuracy using Radial SVM",accuracy_score(y_test,y_pred))
print("Precision using Radial SVM",precision_score(y_test,y_pred))
print("Recall using Radial SVM",recall_score(y_test,y_pred))
print("f1 score using Radial SVM",f1_score(y_test,y_pred))

confusion_matrix(y_test,y_pred)

"""**Among the three SVM, SVM Linear and SVM Radial is having Accuracy of 69 percentage and F1 score 67 percentage**

**Fine Tuning Hyper parameters 'gamma', C and degree for SVM model**
"""

##creating the instance
gammas = [0.1, 1, 10, 100]
for gamma in gammas:
  svm_radial=SVC(kernel='rbf',gamma=gamma)
  svm_radial.fit(x_train,y_train)
  y_pred=svm_radial.predict(x_test)
  print("Accuracy using Radial SVM for gamma=",gamma,":",accuracy_score(y_test,y_pred))

"""**After Fine Tuning the 'gamma' parameter, the Accuracy of Radial SVM has not increased.Accuracy is max for Default gamma value**

**Predicting the Late Delivery**
"""

x_input=[[285,3,8,25,5,2,24,3,4,2016]]

y_pred=svm_linear.predict(x_input)
output=y_pred.item()
if (output==1):
  print("Late Delivery")
else:
  print("Delivery in time")